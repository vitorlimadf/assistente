# Assistente Pessoal com Ollama, OpenRouter, LangGraph, FastAPI e React

Este projeto implementa um assistente pessoal de linguagem natural que roda localmente, combinando:

- [LangGraph](https://github.com/langchain-ai/langgraph) para orquestra√ß√£o de agentes
- [LangChain](https://github.com/langchain-ai/langchain) para ferramentas, mem√≥ria e integra√ß√£o
- [Ollama](https://ollama.com) para execu√ß√£o local de modelos de linguagem (LLMs)
- [FastAPI](https://fastapi.tiangolo.com/) servindo uma API e WebSocket
- [React](https://react.dev/) para a interface web moderna
- [OpenRouter](https://openrouter.ai/) adicionado posteriormente para casos de limita√ß√£o de hardware

---

## Requisitos

- Python 3.10 ou superior  
- [Ollama](https://ollama.com/download) instalado  
- Git instalado  

---

## Instala√ß√£o

### 1. Clone este reposit√≥rio

```bash
git clone https://github.com/vitorlimadf/assistente.git
cd assistente
```

### 2. Crie e ative um ambiente virtual

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate     # Windows
```

### 3. Instale as depend√™ncias

```bash
pip install -r requirements.txt
```

---

## Baixando o Modelo

Antes de executar o assistente, baixe um modelo local com Ollama:

```bash
ollama pull mistral
```

Outros modelos compat√≠veis (falta testar todos):

- `llama3`
- `openchat`
- `gemma`
- `nous-hermes2`

Voc√™ pode alternar o modelo utilizado alterando o nome no c√≥digo ou utilizando uma vari√°vel de ambiente (`LLM_MODEL`).

---

## Executando o Assistente

### 1. (Opcional) Inicie manualmente o servidor do Ollama

```bash
ollama serve
```

Na maioria dos casos, isso ocorre automaticamente em segundo plano.

### 2. Inicie a API com FastAPI

```bash
uvicorn server:app --reload
```

Abra o navegador em [http://localhost:8000](http://localhost:8000). O front-end React √© servido diretamente pela aplica√ß√£o.

---

## Estrutura do Projeto

```
üìÅ seu-repositorio/
‚îú‚îÄ‚îÄ server.py            # API FastAPI com WebSocket
‚îú‚îÄ‚îÄ frontend/            # Aplica√ß√£o React
‚îú‚îÄ‚îÄ agente_graph.py      # L√≥gica do agente com LangGraph + Ollama + OpenRouter
‚îú‚îÄ‚îÄ token_manager        # gerenciamento de tokens de acesso para conectar ao e-mail
‚îú‚îÄ‚îÄ requirements.txt     # Depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md            # Este arquivo
```

---

## Banco de Dados Local

As conversas agora s√£o armazenadas em um arquivo SQLite (`conversations.db`)
na raiz do projeto. O banco √© criado automaticamente na primeira execu√ß√£o.
Caso queira utilizar outro caminho, altere a constante `DB_PATH` em
`conversation_storage.py`.

Os t√≠tulos das conversas s√£o gerados automaticamente pela IA na primeira
resposta. Na barra lateral agora √© poss√≠vel pesquisar tanto pelos t√≠tulos
quanto pelo conte√∫do das mensagens para localizar conversas antigas com
mais facilidade.

Para renomear ou excluir uma conversa, utilize o bot√£o de tr√™s pontinhos (‚ãØ)
alinhado √† direita do t√≠tulo da conversa na barra lateral. Ap√≥s clicar nesse
bot√£o, um pequeno menu √© exibido com as op√ß√µes **Renomear** e **Excluir**. Ao
escolher "Renomear", aparece um campo de texto para definir o novo t√≠tulo; j√°
"Excluir" requer uma confirma√ß√£o antes de remover o chat da lista.

---

## Observa√ß√µes T√©cnicas

A classe `ChatOllama` foi migrada para o pacote `langchain-ollama`. Para evitar avisos de deprecia√ß√£o:

```bash
pip install -U langchain-ollama
```

E utilize:

```python
from langchain_ollama import ChatOllama
```

A interface web possui um bot√£o de microfone para ditar perguntas por voz. A
fala √© transcrita automaticamente antes do envio ao assistente.

---


TODO: Atualizar com informa√ß√µes de como conectar ao e-mail

## Executando os Testes

Para garantir que tudo est√° funcionando corretamente, rode a su√≠te de testes:

```bash
pip install pytest
pytest
```

## Licen√ßa

Este projeto √© open-source e pode ser utilizado, modificado e redistribu√≠do livremente, conforme os termos da licen√ßa inclu√≠da no reposit√≥rio.

Desenvolvido por Vitor Lima (E pelo ChatGPT, √© claro :)
